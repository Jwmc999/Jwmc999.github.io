I"†<h2 id="motivation">Motivation</h2>
<ul>
  <li>The goal of algorithm analysis is to take a block of code and determine the asymptotic run time or asymptotic memory requirements based on various parameters.
    <ul>
      <li>Given an array of size n:
        <ul>
          <li>Merge sort requires $\Theta(n lg n)$ time and $\Theta(n)$ additional memory</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The asymptotic behavior of algorithms indicates the ability to scale</li>
  <li>Suppose that we have two algorithms A and B whose runtimes are $f_{A}(n)=\Theta(n^2)$ and $f_{B}(n)=\Theta(n lg n)$
    <ul>
      <li>For $n=2k$
        <ul>
          <li>$f_{A}(n)=(2k)^2=4k^2$</li>
          <li>$f_{B}(n)=2k lg 2k=2k (lg k+lg 2) = 2k(lg k+1)=2k lg k + 2k$</li>
        </ul>
      </li>
      <li>For $n=10k$
        <ul>
          <li>$f_{A}(n)=(10k)^2=4100k^2$</li>
          <li>$f_{B}(n)=10k lg 10k=10k(lg k+lg 10)\approx 10k lg k+33.2k$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Binary search runs in $\Theta(lg n)$ time:
    <ul>
      <li>Doubling the size <em>n</em> requires one additional search
        <h2 id="machine-instructions">Machine Instructions</h2>
      </li>
    </ul>
  </li>
  <li>Given any processor, it is capable of performing only a limited number of operations</li>
  <li>These operations are called <em>instructions</em></li>
  <li>The collection of instructions is called the <em>instruction set</em>
    <ul>
      <li>The exact set of instructions differs between processors</li>
      <li>MIPS, ARM, x86, 68k, ..</li>
    </ul>
  </li>
  <li>Any instruction runs in a fixed amount of time (an integral number of CPU cycles)</li>
  <li>An example (on the Coldfire): 0x06870000000F
    <ul>
      <li>which adds 15 to the <strong>7th</strong> data register</li>
      <li>As human are not good at hex, this can be programmed in assembly language as: <strong>ADDI.L#$F,D7</strong></li>
    </ul>
  </li>
  <li>Assembly language has an almost one-to-one translation to machine Instructions
    <ul>
      <li>Assembly language is a low-level programming language</li>
    </ul>
  </li>
  <li>Other programming languages are higher-level:
    <ul>
      <li>Fortran, Pascal, MATLAB, Java, C++, C#, ..</li>
    </ul>
  </li>
  <li>The adjective ‚Äúhigh‚Äù refers to the level of abstraction:
    <ul>
      <li>Java, C++, and C# have abstractions such as ‚ÄòObjected Oriented‚Äô</li>
      <li>MATLAB and Fortran have operations which do not map to relatively small number of machine instructions:
-¬ª 1.27^2.9   %1.27**2.9 in Fortran</li>
    </ul>
  </li>
  <li>The C programming language (C++ without objects and other abstractions) can be referred to as a mid-level programming language
    <ul>
      <li>There is abstraction, but the language is closely tied to the standard capabilities</li>
      <li>There is a closer relationship between operators and machine instructions</li>
    </ul>
  </li>
  <li>Consider the operation <strong>a+=b</strong>;
    <ul>
      <li>Assume that the compiler has already has the value of the variable <strong>a</strong> in register <strong>D1</strong> and perhaps <strong>b</strong> is a variable stored at the location stored to the single instruction: <strong>ADD(A1),D1</strong>
        <h2 id="operators">Operators</h2>
      </li>
    </ul>
  </li>
  <li>Because each machine instruction can be executed in a fixed number of cycles, we may assume each operation requires a fixed number of cycles
    <ul>
      <li>The time required for any operator is $\Theta(1)$ including:
        <ul>
          <li>Retrieving/storing variables from memory</li>
          <li>Variable assignment =</li>
          <li>Integer operations  + - * / % ++ ‚Äì</li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>Logical operations  &amp;&amp;</td>
                  <td>¬†</td>
                  <td>!</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>
            <table>
              <tbody>
                <tr>
                  <td>Bitwise operations  &amp;</td>
                  <td>^ ~</td>
                </tr>
              </tbody>
            </table>
          </li>
          <li>Relational operations == != &lt; &lt;= =&gt; &gt;</li>
          <li>Memory allocation and deallocation  new delete
            <h2 id="operations">Operations</h2>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Of these, memory allocation and deallocation are the slowest by a significant factor (e.g. 100 times slower)
    <ul>
      <li>They require communication with the operation system</li>
      <li>This does not account for the time required to call the constructor and destructor
        <ul>
          <li>Note that after memory is allocated, the constructor is run
            <ul>
              <li>The constructor may not run in $\Theta(1)$ time
                <h2 id="blocks-of-operations">Blocks of Operations</h2>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Each operation runs in $\Theta(1)$ time and therefore any fixed number of operations also run in $\Theta(1)$ time.
    <h2 id="blocks-in-sequence">Blocks in Sequence</h2>
  </li>
  <li>Suppose that you have now analyzed a number of blocks of code run in Sequence
```cpp
int* increase_Capacity(int *_array, int _n, int _delta){
int *array_old=_array;
_array=new int[_n+_delta];//$\Theta(1)$</li>
</ul>
:ET